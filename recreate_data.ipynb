{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scripts import scrape_reddit as sr\n",
    "from datetime import datetime\n",
    "from scripts import extract_pairs as ep\n",
    "from scripts import extract_patterns as ept\n",
    "from scripts import link_google_id as gid\n",
    "import swifter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair extraction with base pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape reddit data and create 'banned from' corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to recreate the 'banned from' corpus\n",
    "\n",
    "min_date = int(datetime.strptime('01-10', '%m-%y').timestamp())\n",
    "max_date = int(datetime.strptime('12-20', '%m-%y').timestamp())\n",
    "folder_path = 'banned_from_corpus/'\n",
    "\n",
    "count = sr.scrape_pattern('banned from', '<ent> banned from <plat>', min_date, max_date, folder_path, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to recreate the 'banned from' corpus\n",
    "\n",
    "min_date = int(datetime.strptime('01-18', '%m-%y').timestamp())\n",
    "max_date = min_date + 20000000\n",
    "folder_path = 'test/'\n",
    "\n",
    "count = sr.scrape_pattern('banned from', '<ent> banned from <plat>', min_date, max_date, folder_path, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'banned from' corpus files are pandas dataframe in the following format:\n",
    "\n",
    "text|date|pattern|id|score|nb_comments\n",
    "---|---|---|---|---|---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract entity/platform pairs from the 'banned from' corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banned_from_corpus = ''' Complete here '''\n",
    "\n",
    "regex_entity = \"(.+?)(?:| (?:am|was|is|are|were|got|get|will be|getting|being|has been|have been|been)) banned from\"\n",
    "regex_platform = \"banned from(?: the)? (\\w+)\"\n",
    "\n",
    "banned_from_corpus = banned_from_corpus[banned_from_corpus.text.swifter.apply(lambda x: ep.has_platform(x))] # filter those who don't have the platform name\n",
    "banned_from_corpus['platform'] = banned_from_corpus.text.swifter.apply(lambda x: ep.extract_platform(x, regex_platform))# extract platform\n",
    "banned_from_corpus = banned_from_corpus.dropna() # Remove all lines for which there was no platform\n",
    "banned_from_corpus['entity'] = banned_from_corpus.text.swifter.apply(lambda x: ep.extract_entity(x, regex_entity)) # extract entities \n",
    "banned_from_corpus = banned_from_corpus.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is in the form of:\n",
    "\n",
    "entity | platform | text | date | pattern | id | score | nb_comments\n",
    "---|---|---|---|---|---|---|---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape reddit posts containing the base pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionnary from the base pairs\n",
    "\n",
    "base_pairs = pd.read_csv('results/base_pairs.csv')\n",
    "base_pairs.platform = base_pairs.platform.apply(lambda x: [x])\n",
    "\n",
    "# group by entities\n",
    "base_pairs_dict = dict(base_pairs.groupby('entity').agg({'platform': 'sum'}).reset_index().values)\n",
    "entities = list(base_pairs_dict.keys())\n",
    "base_pairs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_date = int(datetime.strptime('01-10', '%m-%y').timestamp())\n",
    "max_date = int(datetime.strptime('12-20', '%m-%y').timestamp())\n",
    "folder_path = 'posts_base_pairs/'\n",
    "\n",
    "count = 0 # to keep track of the index of the files being written\n",
    "for i, e in enumerate(entities):\n",
    "    count = sr.scrape_pairs(e, base_pairs_dict[e], min_date, max_date, f'{folder_path}{i}/', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is in the form of:\n",
    "\n",
    "entity | platforms | text | date | id | score | nb_comments\n",
    "---|---|---|---|---|---|---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern inference with base pairs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern inference from posts with base pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_base_pairs = '''Complete here'''\n",
    "\n",
    "# Extract patterns from base pairs\n",
    "patterns = posts_base_pairs.swifter.apply(lambda x: ept.extract_patterns(x.text.lower(), x.entity.lower(), x.platforms), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all patterns to lower case and grouped; only patterns with minimum 14 occurences are kept (0.2% of the list)\n",
    "\n",
    "df_patterns = pd.DataFrame(data = [p for sublist in patterns for p in sublist], columns = ['patterns']) # flatten the list of patterns\n",
    "df_patterns['cnt'] = 1\n",
    "df_patterns = df_patterns.groupby('patterns').count().reset_index()\n",
    "df_patterns = df_patterns[df_patterns.cnt >= 14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape posts with inferred patterns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = pd.read_csv('data/raw_patterns.csv', sep = '\\t')\n",
    "folder_path = 'patterns_posts/'\n",
    "min_date = int(datetime.strptime('01-10', '%m-%y').timestamp())\n",
    "max_date = int(datetime.strptime('12-20', '%m-%y').timestamp())\n",
    "\n",
    "count = 0\n",
    "for i, row in patterns.iterrows():\n",
    "    count = sr.scrape_pattern(row.substring, row.pattern, min_date, max_date, folder_path, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is in the form of:\n",
    "\n",
    "text | date | pattern | id | score | nb_comments\n",
    "---|---|---|---|---|---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair extraction from inferred patterns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_platform = lambda pattern, entity_first: f\"{pattern}(\\w+)\" if entity_first else f\"(\\w+){pattern}\"\n",
    "regex_entity = lambda pattern, entity_first: f\"(.+?){pattern}\" if entity_first else f\"{pattern}(.*$)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_inferred_patterns = ''' Complete here with posts containing the infered patterns '''\n",
    "\n",
    "posts_inferred_patterns = posts_inferred_patterns[posts_inferred_patterns.text.swifter.apply(lambda x: ep.has_platform(x))] # filter those who don't have the platform name\n",
    "posts_inferred_patterns['platform'] = posts_inferred_patterns.swifter.apply(lambda x: ep.extract_platform(x.text, regex_platform(x.pattern.replace('<ent>','').replace('<plat>',''), x.pattern[1] == 'e')), axis = 1)# extract platform\n",
    "posts_inferred_patterns = posts_inferred_patterns.dropna() # Remove all lines for which there was no platform\n",
    "posts_inferred_patterns['entity'] = posts_inferred_patterns.swifter.apply(lambda x: ep.extract_entity(x.text, regex_entity(x.pattern.replace('<ent>','').replace('<plat>',''), x.pattern[1] == 'e'), x.pattern[1] == 'e'), axis = 1) # extract entities \n",
    "posts_inferred_patterns = posts_inferred_patterns.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is in the form of:\n",
    "\n",
    "entity | platforms | text | date | id | score | nb_comments\n",
    "---|---|---|---|---|---|---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Pattern precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On s'en fout un peu pour l'instant, pas si important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ban dates extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préciser que après ça il faut aussi le faire à la main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Knowledge Graph linking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = ''' Complete here with (manually and automatically) filtered dataset '''\n",
    "\n",
    "final_dataset['g_id'] = final_dataset.entity.swifter.apply(lambda x: gid.link_google_id(x))\n",
    "final_dataset = final_dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting dataframe needs to be manually checked to verify that the google id collected actually refers to the entity under consideration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marie3.8",
   "language": "python",
   "name": "scratch3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
